{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # regular expressions\nimport matplotlib.pyplot as plt # drawing and visualizing data\nimport seaborn as sns # nicer plotting above matplotlib\nimport nltk # NLP library\nfrom nltk.tokenize import word_tokenize\nfrom tqdm.autonotebook import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-11T07:22:08.395766Z","iopub.execute_input":"2022-03-11T07:22:08.396127Z","iopub.status.idle":"2022-03-11T07:22:10.394043Z","shell.execute_reply.started":"2022-03-11T07:22:08.396096Z","shell.execute_reply":"2022-03-11T07:22:10.392796Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Goal for this notebook is to pull out topics from the conversations in movies","metadata":{}},{"cell_type":"code","source":"conversations = pd.read_csv(\n    \"/kaggle/input/movie-dialog-corpus/movie_conversations.tsv\", \n    sep='\\t', \n    encoding='ISO-8859-2',\n    names = ['charID_1', 'charID_2', 'movieID', 'conversation']\n)\n\n\nlines = pd.read_csv(\n    \"/kaggle/input/movie-dialog-corpus/movie_lines.tsv\", \n    encoding='utf-8-sig', \n    sep='\\t', \n    error_bad_lines=False, \n    header = None,\n    names = ['lineID', 'charID', 'movieID', 'charName', 'text'],\n    index_col=['lineID']\n)\n\ncharacters = pd.read_csv(\n    \"/kaggle/input/movie-dialog-corpus/movie_characters_metadata.tsv\", \n    sep='\\t', \n    header = None,\n    error_bad_lines=False,\n    names = ['charID','charName','movieID','movieName','gender','score'],\n    index_col=['charID']\n)\n\ntitles = pd.read_csv(\n    \"/kaggle/input/movie-dialog-corpus/movie_titles_metadata.tsv\",\n    sep='\\t',\n    header=None,\n    error_bad_lines=False,\n    names=['movieID', 'title', 'year', 'ratingIMDB', 'votes', 'genresIMDB'],\n    index_col=['movieID']\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:10.396265Z","iopub.execute_input":"2022-03-11T07:22:10.396770Z","iopub.status.idle":"2022-03-11T07:22:11.791628Z","shell.execute_reply.started":"2022-03-11T07:22:10.396724Z","shell.execute_reply":"2022-03-11T07:22:11.790293Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"conversations","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:11.793543Z","iopub.execute_input":"2022-03-11T07:22:11.793997Z","iopub.status.idle":"2022-03-11T07:22:11.821246Z","shell.execute_reply.started":"2022-03-11T07:22:11.793941Z","shell.execute_reply":"2022-03-11T07:22:11.820298Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"lines","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:11.822457Z","iopub.execute_input":"2022-03-11T07:22:11.822843Z","iopub.status.idle":"2022-03-11T07:22:11.840282Z","shell.execute_reply.started":"2022-03-11T07:22:11.822809Z","shell.execute_reply":"2022-03-11T07:22:11.838944Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"characters","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:11.844114Z","iopub.execute_input":"2022-03-11T07:22:11.844475Z","iopub.status.idle":"2022-03-11T07:22:11.861473Z","shell.execute_reply.started":"2022-03-11T07:22:11.844439Z","shell.execute_reply":"2022-03-11T07:22:11.860379Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"titles","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:11.863883Z","iopub.execute_input":"2022-03-11T07:22:11.864262Z","iopub.status.idle":"2022-03-11T07:22:11.893888Z","shell.execute_reply.started":"2022-03-11T07:22:11.864230Z","shell.execute_reply":"2022-03-11T07:22:11.892789Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Conversations\nWe need to modify the conversation column to be a proper list, to allow us to query these conversations much easier","metadata":{}},{"cell_type":"code","source":"conversations['conversation'] = conversations['conversation'].map(lambda x: re.findall(r\"\\w+\", x))\n\n# I could not find another way to filter based on the length of the conversation other than creating another column\nconversations['length'] = conversations['conversation'].apply(lambda x: len(x))\n\nconversations.sort_values(by=['length'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:11.895475Z","iopub.execute_input":"2022-03-11T07:22:11.895958Z","iopub.status.idle":"2022-03-11T07:22:12.393670Z","shell.execute_reply.started":"2022-03-11T07:22:11.895921Z","shell.execute_reply":"2022-03-11T07:22:12.392777Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We can see the longest conversation was an 89 line interaction between two characters, but what about the distribution of interactions between characters?","metadata":{}},{"cell_type":"code","source":"print(conversations.length.describe())\nfig, ax = plt.subplots(figsize=(15,5))\nsns.violinplot(ax=ax, x=conversations.length, inner=None)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:12.395240Z","iopub.execute_input":"2022-03-11T07:22:12.395652Z","iopub.status.idle":"2022-03-11T07:22:12.801393Z","shell.execute_reply.started":"2022-03-11T07:22:12.395620Z","shell.execute_reply":"2022-03-11T07:22:12.800588Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# conversation here should be one row of the conversations df. \n# TODO: should probably change this to allow different arguments\ndef view_convo(conversation, characters, movies, lines):\n    charID_1, charID_2, movieID = conversation['charID_1'], conversation['charID_2'], conversation['movieID']\n    \n    char1 = characters.loc[charID_1].charName\n    char2 = characters.loc[charID_2].charName\n    movie = movies.loc[movieID].title\n    \n    convo_header = f\"This conversation was between {char1} and {char2}, from movie : {movie}.\"\n    print(convo_header)\n    print(f\"{'-' * len(convo_header)}\")\n\n    for lineID in conversation.conversation:\n        line = lines.loc[lineID]\n        print(f\"{line.charName} : {line.text}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:12.802532Z","iopub.execute_input":"2022-03-11T07:22:12.802982Z","iopub.status.idle":"2022-03-11T07:22:12.809748Z","shell.execute_reply.started":"2022-03-11T07:22:12.802939Z","shell.execute_reply":"2022-03-11T07:22:12.808572Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"view_convo(conversations.iloc[0], characters, titles, lines)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:12.811145Z","iopub.execute_input":"2022-03-11T07:22:12.811459Z","iopub.status.idle":"2022-03-11T07:22:12.890940Z","shell.execute_reply.started":"2022-03-11T07:22:12.811429Z","shell.execute_reply":"2022-03-11T07:22:12.889544Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Fixing some import issues with movie_lines.tsv\n1. Some lines have no text, but thats ok, we will treat them as a mute reply\n2. Some lines were parsed in such a way that they either did not get tab seperated and the whole row is actually stored in the lineID column\n3. To further complicate \\#2, we have some lineIDs that do not only contain the whole row info, but contain 100+ seperate lines.\n\nTo fix all three (#3 being an extension of \\#2) We will build a malformed dictionary, and loop through this dictionary, fixing the lines DataFrame.\n","metadata":{}},{"cell_type":"code","source":"# this is to identify all the lines that do not have text\nlines[\"type\"] = lines[\"text\"].apply(lambda x: type(x))\n\n# we create our malformed dictionary with the correct lineID as the key, and the value being the full string from which we will correctly\n# populate \nmalformed_index_dict = {line.split('\\t')[0]: line for line in lines[lines.type == type(float(1.))].index}\n\n# the malformed dict has both correctly parsed but empty lines and broken lines that are all stored in lineID. Lets remove the empty lines\nempty_lines = []\nfor k, v in malformed_index_dict.items():\n    if k == v:\n        empty_lines.append(k)\n\n# we remove the empty lines and now \nfor empty_k in empty_lines:\n    malformed_index_dict.pop(empty_k, None)\n    lines.loc[empty_k]['text'] = \" \"\n    lines.loc[empty_k]['type'] = type(\"a\")\n    \n# lets fix the indices first, then loop over again and update the values \nlines.rename(index={v: k for k, v in malformed_index_dict.items()}, inplace=True)\n\n##################################### \n\nfixed = []\n\n# first, lets go ahead and append the lines where the lineID contains all the data for a single line, and in a bad array, store the lineIDs \n# that contain the information for multiple lines\nbad = []\nfor idx, bad_idx in malformed_index_dict.items():\n    if len(bad_idx.split('\\t')[1:] + [type(str)]) > 5:\n        bad.append(idx)\n    else:\n        fixed.append(bad_idx.split('\\t') + [type(\"a\")])\n\n# now fix and append the lineIDs that contain all the data for multiple lines into fixed     \nfor val in bad:\n    for line in malformed_index_dict[val].split(\"\\n\"):\n        if len(line.split('\\t')[1:] + [type(\"a\")]) > 5:\n            fixed.append(line.split('\\t')[:4] + [\"\".join(line.split('\\t')[4:])] + [type(\"a\")])\n        else:\n            fixed.append(line.split('\\t') + [type(\"a\")])\n\n            \n# so now lets make a dataframe out of fixed, and update lines with it\ndf_fixed = pd.DataFrame(fixed, columns=['lineID', 'charID', 'movieID', 'charName', 'text', 'type'])\ndf_fixed.set_index(['lineID'], inplace=True)\n\nlines = pd.concat([df_fixed, lines])\nlines = lines.groupby(lines.index).first().drop(columns=['type'])\n\n# lines = pd.concat([df_fixed, lines]).drop_duplicates(subset=['lineID'])\n# https://stackoverflow.com/questions/63842185/how-to-update-one-pandas-dataframe-with-another-dataframe-update-the-old-data-a","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:12.892691Z","iopub.execute_input":"2022-03-11T07:22:12.893022Z","iopub.status.idle":"2022-03-11T07:22:14.392488Z","shell.execute_reply.started":"2022-03-11T07:22:12.892992Z","shell.execute_reply":"2022-03-11T07:22:14.391616Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Building a vocabulary","metadata":{}},{"cell_type":"markdown","source":"Obligatory word cloud using every single line","metadata":{}},{"cell_type":"code","source":"# Import the wordcloud library\nfrom wordcloud import WordCloud\n# Join the different processed titles together.\nlong_string = ','.join(list(lines.text.values))\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n# Generate a word cloud\nwordcloud.generate(long_string)\n# Visualize the word cloud\nwordcloud.to_image()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:14.393867Z","iopub.execute_input":"2022-03-11T07:22:14.394449Z","iopub.status.idle":"2022-03-11T07:22:27.081198Z","shell.execute_reply.started":"2022-03-11T07:22:14.394413Z","shell.execute_reply":"2022-03-11T07:22:27.080166Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Do we care about the following?\n1. Getting rid of stop words? --- Probably not? \n2. Stemming our words? --- Probably not?\n3. Getting rid of rare words? --- Probably not?\n4. Lower casing everything? --- Yes\n5. Punctuation type? --- Probably not? Just make all the punctuation \".\" for now","metadata":{}},{"cell_type":"code","source":"def process_line(line):\n    line = re.sub(r'[,!?;-]', '.', line)\n    return line.lower()\n    line = word_tokenize(line)\n    # return line\n\ntotal_words = [process_line(line) for line in tqdm(lines.text)]\nprint(len(total_words))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:44:29.331764Z","iopub.execute_input":"2022-03-11T07:44:29.332387Z","iopub.status.idle":"2022-03-11T07:44:30.518878Z","shell.execute_reply.started":"2022-03-11T07:44:29.332349Z","shell.execute_reply":"2022-03-11T07:44:30.517987Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"lines['pText'] = lines['text'].apply(process_line)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:27.090800Z","iopub.execute_input":"2022-03-11T07:22:27.091564Z","iopub.status.idle":"2022-03-11T07:22:28.038073Z","shell.execute_reply.started":"2022-03-11T07:22:27.091489Z","shell.execute_reply":"2022-03-11T07:22:28.036787Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"We have an array of tokens per line. We can choose to append this to the DataFrame for easier access","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:28.039864Z","iopub.execute_input":"2022-03-11T07:22:28.040382Z","iopub.status.idle":"2022-03-11T07:22:28.078731Z","shell.execute_reply.started":"2022-03-11T07:22:28.040330Z","shell.execute_reply":"2022-03-11T07:22:28.077623Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(stop_words='english')\ncount_data = count_vectorizer.fit_transform(lines['pText'])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:28.080241Z","iopub.execute_input":"2022-03-11T07:22:28.080668Z","iopub.status.idle":"2022-03-11T07:22:32.799460Z","shell.execute_reply.started":"2022-03-11T07:22:28.080630Z","shell.execute_reply":"2022-03-11T07:22:32.798314Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"lda = LDA(n_components=5)\nlda.fit(count_data)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:22:32.800630Z","iopub.execute_input":"2022-03-11T07:22:32.800930Z","iopub.status.idle":"2022-03-11T07:34:48.184981Z","shell.execute_reply.started":"2022-03-11T07:22:32.800901Z","shell.execute_reply":"2022-03-11T07:34:48.183691Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, 10)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:34:48.186528Z","iopub.execute_input":"2022-03-11T07:34:48.186814Z","iopub.status.idle":"2022-03-11T07:34:48.265470Z","shell.execute_reply.started":"2022-03-11T07:34:48.186787Z","shell.execute_reply":"2022-03-11T07:34:48.264562Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"each_word = sum(total_words, [])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:44:37.120311Z","iopub.execute_input":"2022-03-11T07:44:37.120747Z","iopub.status.idle":"2022-03-11T07:44:37.139437Z","shell.execute_reply.started":"2022-03-11T07:44:37.120709Z","shell.execute_reply":"2022-03-11T07:44:37.138351Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"freq = nltk.FreqDist(each_word)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:34:48.353052Z","iopub.status.idle":"2022-03-11T07:34:48.353565Z"},"trusted":true},"execution_count":null,"outputs":[]}]}